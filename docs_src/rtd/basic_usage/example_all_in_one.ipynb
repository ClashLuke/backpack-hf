{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample using all extensions\n==============================\n\nBasic example showing how compute the gradient,\nand and other quantities with BackPACK,\non a linear model for MNIST.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by loading some dummy data and extending the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from backpack.utils.examples import load_one_batch_mnist\nfrom torch.nn import CrossEntropyLoss, Flatten, Linear, Sequential\nfrom backpack import backpack, extend\nfrom backpack.extensions import KFAC, KFLR, KFRA\nfrom backpack.extensions import DiagGGNExact, DiagGGNMC, DiagHessian\nfrom backpack.extensions import BatchGrad, SumGradSquared, Variance, BatchL2Grad\n\nX, y = load_one_batch_mnist(batch_size=512)\n\nmodel = Sequential(Flatten(), Linear(784, 10),)\nlossfunc = CrossEntropyLoss()\n\nmodel = extend(model)\nlossfunc = extend(lossfunc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First order extensions\n----------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch gradients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(BatchGrad()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".grad_batch.shape:       \", param.grad_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(Variance()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".variance.shape:         \", param.variance.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second moment/sum of gradients squared\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(SumGradSquared()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L2 norm of individual gradients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(BatchL2Grad()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's also possible to ask for multiple quantities at once\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(BatchGrad(), Variance(), SumGradSquared(), BatchL2Grad()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".grad_batch.shape:       \", param.grad_batch.shape)\n    print(\".variance.shape:         \", param.variance.shape)\n    print(\".sum_grad_squared.shape: \", param.sum_grad_squared.shape)\n    print(\".batch_l2.shape:         \", param.batch_l2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second order extensions\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diagonal of the Gauss-Newton and its Monte-Carlo approximation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(DiagGGNExact(), DiagGGNMC(mc_samples=1)):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".diag_ggn_mc.shape:      \", param.diag_ggn_mc.shape)\n    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KFAC, KFRA and KFLR\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(KFAC(mc_samples=1), KFLR(), KFRA()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".kfac (shapes):          \", [kfac.shape for kfac in param.kfac])\n    print(\".kflr (shapes):          \", [kflr.shape for kflr in param.kflr])\n    print(\".kfra (shapes):          \", [kfra.shape for kfra in param.kfra])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diagonal Hessian\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = lossfunc(model(X), y)\nwith backpack(DiagHessian()):\n    loss.backward()\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(\".grad.shape:             \", param.grad.shape)\n    print(\".diag_h.shape:           \", param.diag_h.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}