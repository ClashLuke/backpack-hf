{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diagonal Gauss-Newton Second order optimizer\n================================================\n\nA simple second-order optimizer with BackPACK on the\n`classic MNIST example from PyTorch\n<https://github.com/pytorch/examples/blob/master/mnist/main.py>`_.\nThe optimizer we implement uses\nuses the diagonal of the GGN/Fisher matrix as a preconditioner,\nwith a constant damping parameter;\n\n\\begin{align}x_{t+1} = x_t - \\gamma (G(x_t) + \\lambda I)^{-1} g(x_t),\\end{align}\n\nwhere\n\n\\begin{align}\\begin{array}{ll}\n        x_t:     & \\text{parameters of the model}                             \\\\\n        g(x_t):  & \\text{gradient}                                            \\\\\n        G(x_t):  & \\text{diagonal of the Gauss-Newton/Fisher matrix at `x_t`} \\\\\n        \\lambda: & \\text{damping parameter}                                   \\\\\n        \\gamma:  & \\text{step-size}                                           \\\\\n    \\end{array}\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get the imports, configuration and some helper functions out of the way first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nfrom backpack import backpack, extend\nfrom backpack.extensions import DiagGGNMC\nfrom backpack.utils.examples import get_mnist_dataloder\nimport matplotlib.pyplot as plt\n\nBATCH_SIZE = 128\nSTEP_SIZE = 0.01\nDAMPING = 1.0\nMAX_ITER = 200\nPRINT_EVERY = 50\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\n\nmnist_loader = get_mnist_dataloder(batch_size=BATCH_SIZE)\n\nmodel = torch.nn.Sequential(\n    torch.nn.Conv2d(1, 20, 5, 1),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2, 2),\n    torch.nn.Conv2d(20, 50, 5, 1),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d(2, 2),\n    torch.nn.Flatten(),\n    torch.nn.Linear(4 * 4 * 50, 500),\n    torch.nn.ReLU(),\n    torch.nn.Linear(500, 10),\n).to(DEVICE)\n\nloss_function = torch.nn.CrossEntropyLoss().to(DEVICE)\n\n\ndef get_accuracy(output, targets):\n    \"\"\"Helper function to print the accuracy\"\"\"\n    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n    return predictions.eq(targets).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing the optimizer\n---------------------\nTo compute the update, we will need access to the diagonal of the Gauss-Newton,\nwhich will be provided by Backpack in the ``diag_ggn_mc`` field,\nin addition to the ``grad`` field created py PyTorch.\nWe can use it to compute the update direction\n\n\\begin{align}(G(x_t) + \\lambda I)^{-1} g(x_t)\\end{align}\n\nfor a parameter ``p`` as\n\n\\begin{align}\\texttt{p.grad / (p.diag_ggn_mc + damping)}\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DiagGGNOptimizer(torch.optim.Optimizer):\n    def __init__(self, parameters, step_size, damping):\n        super().__init__(parameters, dict(step_size=step_size, damping=damping))\n\n    def step(self):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                step_direction = p.grad / (p.diag_ggn_mc + group[\"damping\"])\n                p.data.add_(-group[\"step_size\"], step_direction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running and plotting\n--------------------\nAfter ``extend``-ing the model and the loss function and creating the optimizer,\nthe only difference with a standard PyTorch training loop will be the activation\nof the `DiagGGNMC`` extension using a ``with backpack(DiagGGNMC()):`` block,\nso that BackPACK stores the diagonal of the GGN in the\n``diag_ggn_mc`` field during the backward pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "extend(model)\nextend(loss_function)\noptimizer = DiagGGNOptimizer(model.parameters(), step_size=STEP_SIZE, damping=DAMPING)\n\nlosses = []\naccuracies = []\nfor batch_idx, (x, y) in enumerate(mnist_loader):\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    outputs = model(x)\n    loss = loss_function(outputs, y)\n\n    with backpack(DiagGGNMC()):\n        loss.backward()\n\n    optimizer.step()\n\n    # Logging\n    losses.append(loss.detach().item())\n    accuracies.append(get_accuracy(outputs, y))\n\n    if (batch_idx % PRINT_EVERY) == 0:\n        print(\n            \"Iteration %3.d/%3.d \" % (batch_idx, MAX_ITER)\n            + \"Minibatch Loss %.3f  \" % losses[-1]\n            + \"Accuracy %.3f\" % accuracies[-1]\n        )\n\n    if MAX_ITER is not None and batch_idx > MAX_ITER:\n        break\n\nfig = plt.figure()\naxes = [fig.add_subplot(1, 2, 1), fig.add_subplot(1, 2, 2)]\n\naxes[0].plot(losses)\naxes[0].set_title(\"Loss\")\naxes[0].set_xlabel(\"Iteration\")\n\naxes[1].plot(accuracies)\naxes[1].set_title(\"Accuracy\")\naxes[1].set_xlabel(\"Iteration\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}