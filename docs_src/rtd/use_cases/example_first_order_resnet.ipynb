{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First order extensions with a ResNet\n========================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get the imports, configuration and some helper functions out of the way first.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nfrom backpack import backpack, extend\nfrom backpack.extensions import BatchGrad\nfrom backpack.utils.examples import load_one_batch_mnist\nimport torch.nn.functional as F\n\nBATCH_SIZE = 3\ntorch.manual_seed(0)\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_accuracy(output, targets):\n    \"\"\"Helper function to print the accuracy\"\"\"\n    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n    return predictions.eq(targets).float().mean().item()\n\n\nx, y = load_one_batch_mnist(batch_size=BATCH_SIZE)\nx, y = x.to(DEVICE), y.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyFirstResNet(torch.nn.Module):\n    def __init__(self, C_in=1, C_hid=5, input_dim=(28, 28), output_dim=10):\n        super().__init__()\n\n        self.conv1 = torch.nn.Conv2d(C_in, C_hid, kernel_size=3, stride=1, padding=1)\n        self.conv2 = torch.nn.Conv2d(C_hid, C_hid, kernel_size=3, stride=1, padding=1)\n        self.linear1 = torch.nn.Linear(input_dim[0] * input_dim[1] * C_hid, output_dim)\n        if C_in == C_hid:\n            self.shortcut = torch.nn.Identity()\n        else:\n            self.shortcut = torch.nn.Conv2d(C_in, C_hid, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        x = self.conv2(F.relu(self.conv1(x)))\n        x += residual\n        x = x.view(x.size(0), -1)\n        x = self.linear1(x)\n        return x\n\n\nmodel = extend(MyFirstResNet()).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.zero_grad()\nloss = F.cross_entropy(model(x), y, reduction=\"sum\")\nwith backpack(BatchGrad()):\n    loss.backward()\n\nprint(\"{:<20}  {:<30} {:<30}\".format(\"Param\", \"grad\", \"grad (batch)\"))\nprint(\"-\" * 80)\nfor name, p in model.named_parameters():\n    print(\n        \"{:<20}: {:<30} {:<30}\".format(name, str(p.grad.shape), str(p.grad_batch.shape))\n    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_to_check = 1\nx_to_check = x[sample_to_check, :].unsqueeze(0)\ny_to_check = y[sample_to_check].unsqueeze(0)\n\nmodel.zero_grad()\nloss = F.cross_entropy(model(x_to_check), y_to_check)\nloss.backward()\n\nprint(\"Do the individual gradient match?\")\nfor param_id, (name, p) in enumerate(model.named_parameters()):\n    print(\n        name, torch.allclose(p.grad_batch[sample_to_check, :], p.grad, atol=1e-7),\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}