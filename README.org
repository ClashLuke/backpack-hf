# -*- coding: utf-8 -*-
#+STARTUP: indent
#+AUTHOR: Felix Dangel
# export using org-gfm-export-to-markdown

* Backpropagation extensions (~bpexts~) for ~PyTorch~
~bpexts~ provides extended ~PyTorch~ layers capable of computing different quantities than just the gradient.
Currently, feedforward (fully-connected and convolutional) neural networks are supported.

Modules are designed in such a way that they can be used as drop-in replacement to the corresponding ~torch.nn.Module~ class.
** Overview
 Very briefly, the following quantities can be computed:
 - First-order information
   - Batchwise gradients (~bpexts.gradient~)
   - Sum of squared gradients (~bpexts.sumgradsquared~)
 - Second-order information (for more details, consult [[hbp:paper]])
   - Approximate block-diagonal curvature matrices obtained by Hessian backpropagation (~bpexts.hbp~)
   - Exact block-diagonal curvature matrix-vector products (~bpexts.cvp~)
** Applications
 The main motivation of ~bpexts~ is to preserve ~PyTorch~'s  modular structure.
 Algorithms that require more information than just the gradient, for instance

 - Variance-adapted first-order optimization methods
 - Second-order methods that use a block-diagonal curvature estimate
 
 can thus be formulated in a more elegant fashion.

** Getting started
*** Installation
   1) Clone the repository
      #+BEGIN_SRC bash:
      git clone https://github.com/f-dangel/bpexts.git
      #+END_SRC
   2) Change into the directory
      #+BEGIN_SRC bash:
      cd bpexts/
      #+END_SRC
   3) Install dependencies and ~bpexts~
      #+BEGIN_SRC bash:
      pip3 install -r ./requirements.txt
      pip3 install .
      #+END_SRC
   4) (*Optional*, for reproducing experiments of [[hbp:paper]]) Install requirements to run our experiments
      #+BEGIN_SRC bash:
      pip3 install -r ./requirements_exp.txt
      #+END_SRC

  Congratulations! You should now be able to call ~import bpexts~ in a ~python3~ session.

*** (Optional) Verify installation
   If ~pytest~ is installed on your system, you can run the tests in the repository directory by
   #+BEGIN_SRC bash:
   pytest -v tests/
   #+END_SRC

*** Check out the tutorials
   One main goal of ~bpexts~ is to simplify the computation of quantities in the backward pass.
   You can find explanations on how to use the code in the [[file:./examples][examples]] directory.

   Available tutorials:
   - First-order information
     - TODO
   - Second-order information
     - [[file:examples/hbp/01_single_layer_mnist.md][Second-order optimization with HBP for a single layer on MNIST]]
     - [[file:examples/hbp/02_sequential_cifar10.md][Second-order optimization with HBP for a sequential model on CIFAR-10]]
   - Auxiliary
     - TODO

   We plan to add further tutorials in the future.
   
** Functionality 
  - ~bpexts.gradient~ :: Batch gradients
    - For a mini-batch over N samples, compute the individual gradients g_n of the gradient g = \sum_n g_n
  - ~bpexts.sumgradsquared~ :: Sum of squared gradients
    - For a mini-batch over N samples, instead of computing the gradient (the sum of the gradient of individual samples g = \sum_n g_n), computes the sum of the individual gradients, squared element-wise g_2 = \sum_n g_n \odot g_n (where \odot indicates element-wise multiplication)
    - Given the gradient g and the sgs g_2 this makes it easy to compute the element-wise variance of the gradient over the mini-batch as g_2 - g \odot g  
  - ~bpexts.hbp~ :: Hessian backpropagation (see [[hbp:paper]] for details)
    - Approximate the block-diagonal of different curvature matrices
    - Provides multiplication with the approximate blocks of
      - Hessian (H)
      - Generalized Gauss-Newton matrix (GGN)
      - Positive-curvature Hessian (PCH)
    - Different approximation modes
  - ~bpexts.cvp~ :: Curvature matrix-vector products (see [[hbp:paper]] for details)
    - Provides *exact* multiplication with the diagonal blocks of
      - Hessian (H)
      - Generalized Gauss-Newton matrix (GGN)
      - Positive-curvature Hessian (PCH)
  - ~bpexts.optim~ :: Optimizers
    - Implements conjugate gradients and the Newton-style optimizer used in [[hbp:paper]]
** Related work
   - <<hbp:paper>> [[[hbp:paper]]] Dangel, F. and Hennig, P.: [[https://arxiv.org/abs/1902.01813][A Modular Approach to Block-diagonal Hessian Approximations for Second-order Optimization]] (2019) 
    - The work presents an extended backpropagation procedure, referred to as *Hessian backpropagation (HBP)*,
      for computing curvature approximations of feedforward neural networks.
    - To **reproduce the experiment** (Figure 5) in the paper, we recommend using our script.
      A step-by-step instruction is given in the [[file:examples/2019_02_dangel_hbp/README.rst][README]] file in [[file:examples/2019_02_dangel_hbp/][~examples/2019_02_dangel_hbp/~]].
** Developer notes                                                 :noexport:
  This section contains additional information for developers.
*** Run tests before committing
   Copy the ~pre-commit~ file to your ~.git/hooks/~ directory.
*** Set up a virtual environment with ~virtualenv~
     - Set up a virtual environment with
       #+BEGIN_SRC bash:
       virtualenv --python=/usr/bin/python3 .venv
       #+END_SRC
       .. code:: console
     - Activate it
       #+BEGIN_SRC bash:
       source .venv/bin/activate
       #+END_SRC
     - Install dependencies (also these for development/experiments)
       #+BEGIN_SRC bash:
       pip3 install -r ./requirements.txt
       pip3 install -r ./requirements_exp.txt
       #+END_SRC
     - Install the library (in editable mode)
       #+BEGIN_SRC bash:
       pip3 install --editable .
       #+END_SRC
     - Deactivate the virtual environment by typing
       #+BEGIN_SRC bash:
       deactivate
       #+END_SRC
     - Run tests manually
       #+BEGIN_SRC bash:
       pytest -v bpexts
       pytest -v exp
       #+END_SRC
